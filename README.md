# âš¾ Bullpen Management Simulator (BMS)

The Bullpen Management Simulator (BMS) is an endâ€‘toâ€‘end machineâ€‘learning application that recommends the most appropriate reliever to bring into a game given the **current game state** and each candidateâ€™s rest/fatigue, handedness and platoon matchâ€‘up.  It combines modern sabermetric principles with a simple user interface and full MLOps packaging so that you can train, serve, monitor and improve the model just like a production service.

## ğŸ’¡ Motivation

Traditional baseball statistics (ERA, saves, holds) donâ€™t reflect the contextual complexity facing a manager when choosing a reliever.  **TomÂ Tangoâ€™s book â€œTheÂ Bookâ€** showed that run expectancy and leverage matter more than raw stats, and that situational decisions cannot be evaluated in isolation.  Modern run value metrics, such as those on BaseballÂ Savant and FanGraphs, calculate cumulative pitch values from run expectancy matrices, but they are geared toward batters and arenâ€™t integrated into an interactive decision tool for bullpen management.  Existing WAR models (FIPâ€‘basedÂ WAR, rWAR, DRAâ€‘basedÂ WARP) estimate a pitcherâ€™s overall season value but do not tell you who to bring into a highâ€‘leverage situation.  Technologies such as TrackMan and Rapsodo provide rich pitchâ€‘tracking data, but coaches still need a structured way to apply that data to game decisions.

This project was born from a frustration with the wealth of stats available to fans and analysts and the lack of a simple tool that answers the question: **â€œWho should pitch next?â€**  As a baseball fan obsessed with how pitchers control the game, I wanted to build a system that translates pitchâ€‘level data and context into actionable insights for managers, coaches, analysts and fans.

## ğŸš€ What it does

- **Predicts expected runs** allowed over the next `K` batters (default three) using a supervised regression model trained on historical playâ€‘byâ€‘play data.
- **Recommends the reliever** with the lowest adjusted expected runs, applying penalties for backâ€‘toâ€‘back days, high pitch counts in the previous outing or unavailability.
- **Explains its reasoning** by returning both the raw predicted value and the penalty components for each candidate.
- **Offers an interactive web UI** built with Streamlit so that users can tweak game state parameters and bullpen options and see the recommendation instantly.
- **Implements MLOps best practices**: MLflow for experiment tracking and model registry, Prometheus for service metrics, Evidently for drift detection, GreatÂ Expectations for data quality checks, and GitHubÂ Actions for CI.
- **Ships as a containerised microservice** with a FastAPI backâ€‘end and a separate UI container, ready for deployment on platforms such as GoogleÂ CloudÂ Run or AWSÂ ECS.

## ğŸ§  How it works

### Target and Model

The core problem is framed as a **regression**: given a game state and reliever characteristics, estimate the **expected number of runs** the opposing team will score over the next `K` batters (or until the inning ends).  This is a more stable and interpretable target than win probability because runs translate directly into win outcomes and can be summed over multiple situations.  The model uses a **GradientÂ BoostingÂ Regressor** (via scikitâ€‘learn) as a strong baseline for tabular data with limited features and moderate nonâ€‘linearities.  Gradientâ€‘boosting models handle interactions (e.g. the effect of runners on base and inning) well, are easy to fit on relatively small datasets, and can be interpreted with tools like SHAP.

Features include:

* **Game context**: number of outs, base/out state (encoded as runners on base), inning, score differential, a simple leverage proxy (late inning indicator Ã— close game indicator), home/away and park identifier.
* **Matchâ€‘up/platoon**: whether the relieverâ€™s handedness matches the expected batter handedness (estimated with a simple segment indicator: top, middle or bottom of the lineup).
* **Reliever fatigue**: days since last outing and pitch count in the previous outing.
* **Optional override**: future extensions might incorporate batter quality, precise leverage index or pitchâ€‘tracking metrics (velocity, movement), but these are outside the current scope for simplicity.

During recommendation, the model prediction is adjusted by **usage penalties**: relievers get a positive penalty if they pitched the day before or threw more than 20 pitches in their last outing, and an infinite penalty if they are marked unavailable.  The reliever with the lowest penalized expected runs is recommended.

### Data and Labeling

The model is trained on pitchâ€‘level data pulled from Statcast via the **pybaseball** library.  For each reliever appearance, we derive features from the game state just before they enter and compute the **actual runs allowed** over the next `K` batters.  This is similar to run expectancy values used in TomÂ Tangoâ€™s book but tailored for bullpen management.  A separate script prepares the dataset, computes features and labels, and logs the training run to MLflow.

### MLOps and Path to Production

- **Experiment tracking and model registry** with MLflow: every training run records parameters (e.g. number of estimators, depth) and performance metrics (e.g. validation MAE) and saves the trained model artifact.  The best model is registered and loaded by the API.
- **Service metrics** using Prometheus: the API exposes `/metrics` that reports request counts, latency and online mean absolute error (mae).  These metrics can be scraped by Prometheus and visualised in Grafana.
- **Data and model drift** detection with Evidently: a nightly Prefect job loads recent predictions and outcomes, compares feature distributions and performance against a reference window, and writes an HTML report while updating Prometheus gauges.
- **Data quality** checks with GreatÂ Expectations: ingestion scripts validate that numeric features are within expected ranges and that categorical fields contain valid values.  If validation fails, the batch is rejected.
- **Continuous integration**: GitHubÂ Actions runs code formatting, linting, static type checking and smoke tests on every push.
- **Containerization**: separate Dockerfiles for the API and UI.  A `docker-compose.yml` orchestrates both services for local development.  Cloudâ€‘native deployment instructions are included in the README.

## ğŸ“¦ Repository structure

```
bullpen-management-simulator/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ .github/workflows/ci.yml
â”œâ”€ bms/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py
â”‚  â”œâ”€ domain.py
â”‚  â”œâ”€ features.py
â”‚  â”œâ”€ model_expected_runs.py
â”‚  â”œâ”€ recommender.py
â”œâ”€ api/
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ schemas.py
â”‚  â”œâ”€ metrics.py
â”‚  â””â”€ Dockerfile
â”œâ”€ ui_streamlit/
â”‚  â”œâ”€ app.py
â”‚  â””â”€ Dockerfile
â”œâ”€ ops/
â”‚  â”œâ”€ drift_job.py
â”‚  â””â”€ gx_checks.py
â”œâ”€ docker-compose.yml
â””â”€ models/
    â””â”€ (trained model artifacts)
```

## ğŸ‘¥ User stories

* **Manager/BenchÂ Coach**: â€œGiven men on first and third with one out in the eighth and our top relievers spent, who minimizes expected runs over the next three batters?â€
* **PitchingÂ Coach**: â€œIf we avoid using our closer today after he threw 25 pitches last night, what is the cost in expected runs and who should we use instead?â€
* **Frontâ€‘office Analyst**: â€œHow do penalty adjustments (rest/fatigue) versus pure matchâ€‘up change the recommendation?  Which of our relievers is most efficient in high leverage?â€
* **Fan/Broadcaster**: â€œI want to interactively explore how different game states and bullpen choices affect the outcome.â€

## ğŸ§ª Running the project

### Local development

1. **Install dependencies**:

   ```bash
   python3 -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Train a demo model** (using synthetic data or a small Statcast sample):

   ```bash
   python scripts/train_demo.py
   ```

   This generates `models/bms.joblib` and logs the run to MLflow.

3. **Run API and UI locally** in separate terminals:

   ```bash
   API_PORT=8080 uvicorn api.main:app --host 0.0.0.0 --port 8080 --reload
   ```

   ```bash
   API_URL=http://localhost:8080 streamlit run ui_streamlit/app.py
   ```

   Navigate to `http://localhost:8501` to interact with the simulator.

### Docker compose

To launch both services together:

```bash
docker compose up --build
```

The API will be available at `http://localhost:8080` and the UI at `http://localhost:8501`.

### Cloud deployment

1. Build and push the API and UI images:

   ```bash
   gcloud builds submit api --tag gcr.io/PROJECT_ID/bms-api
   gcloud builds submit ui_streamlit --tag gcr.io/PROJECT_ID/bms-ui
   ```

2. Deploy to Cloud Run:

   ```bash
   gcloud run deploy bms-api --image gcr.io/PROJECT_ID/bms-api --region=us-central1 --allow-unauthenticated
   gcloud run deploy bms-ui --image gcr.io/PROJECT_ID/bms-ui --region=us-central1 --allow-unauthenticated \
     --set-env-vars API_URL=https://bms-api-<hash>-uc.a.run.app
   ```

## ğŸ“– References

* TomÂ Tango et al., *TheÂ Book: Playing the Percentages in Baseball*.  Chapters on run expectancy and game state emphasise the importance of leverage and context.
* BaseballÂ Savantâ€™s run value metric description: run values are cumulative sums of pitch outcomes relative to run expectancy.
* FanGraphs on different pitching WAR models: comparisons of fWAR (FIPâ€‘based), rWAR (runs allowed) and DRAâ€‘based WARP.
* Pitch tracking technology overview (Rapsodo, TrackMan, etc.): describes the data available to coaches and scouts.
